{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Mining!\n",
    "\n",
    "The problem - How do you determine which items belong together from data? \n",
    "For example imagine you are running a store and you want to know what items are frequently bought together - how would you go about this in an automated and systematic way?\n",
    "\n",
    "Apriori algorithm\n",
    "* pro: Easy to implement, easy to parallelize\n",
    "* con: Computationally expensive\n",
    "\n",
    "\n",
    "Market basket analysis\n",
    " * Better shop layout\n",
    " * Adverse effects of combining medicine\n",
    "\n",
    "Association rules are in the `if ... then ...` format. E.g:\n",
    "\n",
    "> **If** a customer buys tomatoes and carrots **then** they buy tofu.\n",
    "\n",
    "In formal logic this is called an implication, and is generally denoted with an arrow, so the statement above could be written:\n",
    "\n",
    "> `{tomatoes, carrots} -> tofu`\n",
    "\n",
    "Association mining in this case might lead us to the conclusion that 'tofu' should be considered as a vedgetable, rather than as an oriental item, as it it is frequently bought by vegetarians.\n",
    "\n",
    "___\n",
    "\n",
    "Important metrics in apriori are:\n",
    "* Support - of an item set X is the proportion of observations in the dataset where X occurs\n",
    "    * $supp(X) = \\frac{\\text{# observations with X}}{\\text{Total # transactions}}$\n",
    "* Confidence - How strong is the rule? I.e. the confidence for `X->Y` is the likelihood that `Y` is purchased, if `X` is purchased. \n",
    "This is the same as the conditional probability.\n",
    "It is defined as follows:\n",
    "    * $conf(X \\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X)}$\n",
    "* Lift\n",
    "    * If the support for `Y` is very high, then the confidence of `X->Y` might not be very informative. \n",
    "    Lift corrects for this.\n",
    "    * $lift(X\\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X) supp(Y)}$\n",
    "* Conviction\n",
    "    * Conviction is a measure of how much better than chance an association is.\n",
    "    * $conv(X \\rightarrow Y) = \\frac{1-supp(Y)}{1-conf(X->Y)}$\n",
    "\n",
    "\n",
    "Example work\n",
    "1. Load data\n",
    "1. Create itemsets of one-hot DataFrame\n",
    "1. Determine minimum support (and confidence) threshold\n",
    "    1. Some data exploration \n",
    "1. Find all items that satisfies this support threshold\n",
    "1. Find all combinations of the items that satisfy the support threshold. You can use arbitrarily large sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes matplotlib plots work better with Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the necessary libraries. \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pprint import pprint\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "For this exercise we will be working with classical shopping-cart data.\n",
    "The cell below loads the data and prints the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that data and data path is present\n",
    "data_folder = \"../data/\"\n",
    "data_filename = \"shopping.json\"\n",
    "assert os.path.isdir(data_folder) and os.path.exists(data_folder + data_filename), 'Data not found. Make sure to have the most recent version!'\n",
    "\n",
    "with open(data_folder + data_filename) as f:\n",
    "    data_raw = json.load(f)\n",
    "\n",
    "print('Number of observations', len(data_raw))\n",
    "pprint(data_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Transforming the Data\n",
    "> Estimated task time: 10 min\n",
    "\n",
    "The first thing we want to do is transform the data into a one-hot matrix.\n",
    "A one-hot matrix is a matrix where each column represent an item, e.g. `ice cream`, and each row represents a transaction.\n",
    "`0` indicates the item wasn't bought, and `1` indicates that it was.\n",
    "\n",
    "Your task is to\n",
    "1. Create a list of all the feature names, but remove the uninformative ones, i.e. the ones in `bad_feature_names`\n",
    "1. Create a one-hot numpy array of the data, and name it `data`\n",
    " * Note: If an item occurs twice in a transaction we still just want to have a `1` in the one-hot matrix.\n",
    " * Hint: The array should have shape `(1499, 37)`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = len(data_raw)\n",
    "bad_feature_names = ['', 'all- purpose']\n",
    "\n",
    "data = None ## YOUR CODE HERE \n",
    "\n",
    "# Make a list of all the items\n",
    "all_items = []\n",
    "for i in range(num_obs):\n",
    "    all_items += data_raw[i]['items']\n",
    "    \n",
    "# Find the unique item names\n",
    "features_names = list(np.unique(all_items))\n",
    "\n",
    "# Remove bad item names\n",
    "for bad_feature in bad_feature_names:\n",
    "    features_names.remove(bad_feature)\n",
    "\n",
    "print('feature_names')\n",
    "print(features_names)\n",
    "print()\n",
    "# Create a one-hot matrix\n",
    "num_features = len(features_names)\n",
    "data = np.zeros([num_obs, num_features])\n",
    "for i in range(num_obs):\n",
    "    for item in data_raw[i]['items']:\n",
    "        if item not in bad_feature_names:\n",
    "            j = features_names.index(item)\n",
    "            data[i,j] = 1\n",
    "\n",
    "print('The data looks like this now:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Compute the Support\n",
    "\n",
    "> Estimated task time: 10 min\n",
    "\n",
    "Now we want to compute the support of each of the individual items, and visualize it.\n",
    "\n",
    "Your job:\n",
    "1. Finish the `compute_support` function - which computes the support for one combination of items. \n",
    " * E.g. `(0,1,2)` would be `{'aluminum foil', 'bagels', 'beef',}`, which occurs 7.20% of the time.\n",
    "1. Compute the support of each of the items individually - i.e. how frequent are each item.\n",
    "1. Visualize the support as a histogram.\n",
    "    * Hint: `plt.xticks` can be used to rotate the labels on the x-axis for a prettier plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_support(X, combi):\n",
    "    \"\"\" Given a one-hot array X, and a tuple, combi, indicating the columns of interest\n",
    "        compute_support returns the support of the elements in combi.\n",
    "        \n",
    "        The support of an item set X is the proportion of observations in the dataset where X occurs.\n",
    "        $supp(X) = \\frac{\\text{# observations with X}}{\\text{Total # transactions}}$\n",
    "    \"\"\"\n",
    "    pass\n",
    "    ## YOUR CODE HERE\n",
    "    rule = X[:, combi].all(axis=1)\n",
    "    support = rule.sum() / X.shape[0]\n",
    "    return support\n",
    "\n",
    "print('Support for (0,1,2):', compute_support(data, (0,1,2)))\n",
    "\n",
    "support = []\n",
    "for i in range(data.shape[1]):\n",
    "    support.append(compute_support(data, (i,)))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "g = sns.barplot(x=features_names, y=support, color='b')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Creating Combinations\n",
    "> Estimated task time: 10 min\n",
    "\n",
    "Here we want to implement and test a helper function `candidate_generation`.\n",
    "The function takes a `NxM` numpy array of item sets.\n",
    "`N` indicates the number of item sets, and `M` indicates the length.\n",
    "The function should then produce a [generator](https://medium.freecodecamp.org/how-and-why-you-should-use-python-generators-f6fb56650888) that generates all relevant new item set combinations of length `M+1`.\n",
    "\n",
    "Your task:\n",
    "1. Fill out the function `candidate_generation`. The output must be as a tuple.\n",
    " * Note: Watch out you don't create duplicates! We don't care about the order, so `(0, 1)` is the same as `(1, 0)`.\n",
    " * Hint: It is a good idea to start by finding all the unique values in old_combinations.\n",
    "1. Run the test in the next cell, and make sure that you get the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def candidate_generation(old_combinations):\n",
    "    \"\"\" Input: An NxM array of item sets, \n",
    "            N: number of combinations\n",
    "            M: length of each item set\n",
    "        Output: A generator that produces all possible combinations \n",
    "    \"\"\"\n",
    "    \n",
    "    ## YOUR CODE HERE \n",
    "    items_types_in_previous_step = np.unique(old_combinations.flatten())\n",
    "    \n",
    "    for old_combination in old_combinations:\n",
    "        max_combination = max(old_combination)\n",
    "        for item in items_types_in_previous_step:\n",
    "            if item > max_combination:\n",
    "                res = tuple(old_combination) + (item,)\n",
    "                yield res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below serves as a test of `candidate_generation`. \n",
    "If your function is written correctly the output shold be as follows:\n",
    "\n",
    "    [[0 1 2 3]]\n",
    "\n",
    "    [[0 0 0 1 1 2]\n",
    "     [1 2 3 2 3 3]]\n",
    "\n",
    "    [[0 0 0 1]\n",
    "     [1 1 2 2]\n",
    "     [2 3 3 3]]\n",
    "\n",
    "    [[0]\n",
    "     [1]\n",
    "     [2]\n",
    "     [3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of candidate_generation\n",
    "comb = np.array([[0],[1],[2],[3]])\n",
    "print(comb.T)\n",
    "print()\n",
    "\n",
    "new_comb = []\n",
    "for c in candidate_generation(comb):\n",
    "    new_comb.append(c)\n",
    "comb = np.array(new_comb)\n",
    "\n",
    "print(comb.T)\n",
    "print()\n",
    "\n",
    "new_comb = []\n",
    "for c in candidate_generation(comb):\n",
    "    new_comb.append(c)\n",
    "comb = np.array(new_comb)\n",
    "print(comb.T)\n",
    "print()\n",
    "\n",
    "new_comb = []\n",
    "for c in candidate_generation(comb):\n",
    "    new_comb.append(c)\n",
    "comb = np.array(new_comb)\n",
    "print(comb.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Finding the Right Sets \n",
    "> Estimated task time: 25 min\n",
    "\n",
    "In this task we will find all the sets that have sufficient support.\n",
    "\n",
    "\n",
    "Your task:\n",
    "1. Finish the code below using what you have already done in the previous tasks. Each iteration of the loop should:\n",
    " * Generate all relevant combinations.\n",
    " * For each combination compute the support\n",
    " * If the support is larger than `min_support` save the item set and the support in `itemset_dict` and `support_dict`.\n",
    "1. Convert your results, `itemset_dict` and `support_dict`, into a `pandas` dataframe.\n",
    " * Use `reset_index` to fix the indexes (remove the unwanted 'history')\n",
    " * Convert the item sets to `frozenset`. `frozenset` is a [immutable data structure for sets](https://www.programiz.com/python-programming/methods/built-in/frozenset) that enables common set operations like union, intersection, difference (some of which may come in handy later *wink wink*)\n",
    "1. Find an appropriate minimum support (subjective). Run the code several times, and see how changing this value changes the outcome.\n",
    " * You may choose a maximum length as well if you want to.\n",
    "1. Print the results using the actual feature names - how does it look? Any surprises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support = 0.3\n",
    "\n",
    "# Compute / re-compute support, and set it up as a dictionary \n",
    "support = []\n",
    "for i in range(data.shape[1]):\n",
    "    support.append(compute_support(data, (i,)))\n",
    "support = np.asarray(support)\n",
    "support_dict = {1: support[support >= min_support]}\n",
    "\n",
    "# Setup all the attomic items in a dict.\n",
    "ary_col_idx = np.arange(data.shape[1])\n",
    "itemset_dict = {1: ary_col_idx[support >= min_support].reshape(-1, 1)}\n",
    "rows_count = float(data.shape[0])\n",
    "\n",
    "itemset_len = 1\n",
    "while itemset_len:\n",
    "    pass\n",
    "    \n",
    "    ## YOUR CODE HERE \n",
    "    next_itemset_len = itemset_len + 1\n",
    "    combin = candidate_generation(itemset_dict[itemset_len])\n",
    "    frequent_items = []\n",
    "    frequent_items_support = []\n",
    "\n",
    "    for c in combin:\n",
    "        support = compute_support(data, c)\n",
    "        \n",
    "        if support >= min_support:\n",
    "            frequent_items.append(c)\n",
    "            frequent_items_support.append(support)\n",
    "\n",
    "    if frequent_items:\n",
    "        itemset_dict[next_itemset_len] = np.array(frequent_items)\n",
    "        support_dict[next_itemset_len] = np.array(frequent_items_support)\n",
    "        itemset_len = next_itemset_len\n",
    "    else:\n",
    "        itemset_len = 0\n",
    "\n",
    "\n",
    "all_res = []\n",
    "for k in sorted(itemset_dict):\n",
    "    support = pd.Series(support_dict[k])\n",
    "    itemsets = pd.Series([frozenset(i) for i in itemset_dict[k]])\n",
    "\n",
    "    res = pd.concat((support, itemsets), axis=1)\n",
    "    all_res.append(res)\n",
    "\n",
    "res_df = pd.concat(all_res)\n",
    "res_df.columns = ['support', 'itemsets']\n",
    "res_df = res_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Print results in a human readable manner\n",
    "for i in range(37, len(res_df)):\n",
    "    print([features_names[i] for i in res_df.itemsets[i]], 'has support', res_df.support[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Confidence, Lift, Conviction\n",
    "> Estimated task time: 10 min\n",
    "\n",
    "We now\n",
    "TEST IT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence():\n",
    "    \"\"\" The confidence for `X->Y` is the likelihood that `Y` is purchased, if `X` is purchased. \n",
    "        This is the same as the conditional probability.\n",
    "        $conf(X \\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X)}$\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def compute_lift():\n",
    "    \"\"\" \n",
    "        $lift(X\\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X) supp(Y)}$        \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def compute_conviction():\n",
    "    \"\"\" How much better than chance is this association?\n",
    "        \n",
    "        $conv(X \\rightarrow Y) = \\frac{1-supp(Y)}{1-conf(X->Y)}$\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "## YOUR CODE HERE\n",
    "def compute_confidence(supAC, supA):\n",
    "    \"\"\" The confidence for `X->Y` is the likelihood that `Y` is purchased, if `X` is purchased. \n",
    "        This is the same as the conditional probability.\n",
    "        $conf(X \\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X)}$\n",
    "    \"\"\"\n",
    "    return supAC / supA\n",
    "\n",
    "def compute_lift(supAC, supA, supC):\n",
    "    \"\"\" \n",
    "        $lift(X\\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X) supp(Y)}$        \n",
    "    \"\"\"\n",
    "    conf = compute_confidence(supAC, supA)\n",
    "    return conf / supC\n",
    "    \n",
    "\n",
    "def compute_conviction(supAC, supA, supC):\n",
    "    \"\"\" How much better than chance is this association?\n",
    "        \n",
    "        $conv(X \\rightarrow Y) = \\frac{1-supp(Y)}{1-conf(X->Y)}$\n",
    "    \"\"\"\n",
    "\n",
    "    conf = compute_confidence(supAC, supA)\n",
    "    conviction = np.empty(conf.shape, dtype=float)\n",
    "    \n",
    "#     if not len(conviction.shape):\n",
    "#         conviction = conviction[np.newaxis]\n",
    "#         confidence = confidence[np.newaxis]\n",
    "#         sAC = sAC[np.newaxis]\n",
    "#         sA = sA[np.newaxis]\n",
    "#         sC = sC[np.newaxis]\n",
    "        \n",
    "    conviction[:] = np.inf\n",
    "    conviction[conf < 1.] = ((1. - sC[conf < 1.]) /\n",
    "                            (1. - conf[conf < 1.]))\n",
    "\n",
    "    return conviction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Association Rules\n",
    "> Estimated task time: 25 min\n",
    "\n",
    "Your tasks\n",
    "1. Write a for-loop that for each set of size 2 or larger\n",
    " * computes all possilbe association rules (e.g. {A, B, C} becomes {{A -> B, C}, {A, B -> C}, {A, C -> B}, {B, C -> A})\n",
    " * for each association rule compute the score (`score_function`). \n",
    " * If the score is larger than the threshold save the support for the entire set, the antecedent, and the concequent.\n",
    "1. Create a pandas dataframe with all the metrics, and have a look at it.\n",
    "1. Experiment with different selection metrics and thresholds. What gives results that look reasonable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_function = lambda supAC, supA, supC: compute_confidence(supAC, supA)\n",
    "min_threshold = 0.3\n",
    "\n",
    "## YOUR CODE HERE\n",
    "keys = res_df['itemsets'].values\n",
    "values = res_df['support'].values\n",
    "\n",
    "frozenset_vect = np.vectorize(lambda x: frozenset(x))\n",
    "frequent_items_dict = dict(zip(frozenset_vect(keys), values))\n",
    "\n",
    "# prepare buckets to collect frequent rules\n",
    "rule_antecedents = []\n",
    "rule_consequents = []\n",
    "rule_supports = []\n",
    "\n",
    "# iterate over all frequent itemsets\n",
    "for k in frequent_items_dict.keys():\n",
    "    supAC = frequent_items_dict[k]\n",
    "\n",
    "    # find all possible combinations\n",
    "    for idx in range(len(k)-1, 0, -1):\n",
    "        # of antecedent and consequent\n",
    "        for c in combinations(k, r=idx):\n",
    "            antecedent = frozenset(c)\n",
    "            consequent = k.difference(antecedent)\n",
    "\n",
    "            supA = frequent_items_dict[antecedent]\n",
    "            supC = frequent_items_dict[consequent]\n",
    "\n",
    "            confidence = None\n",
    "            lift = None\n",
    "            conviction = None\n",
    "            score = 1\n",
    "            if score >= min_threshold:\n",
    "                rule_antecedents.append(antecedent)\n",
    "                rule_consequents.append(consequent)\n",
    "                rule_supports.append([supAC, supA, supC])\n",
    "\n",
    "\n",
    "# generate metrics\n",
    "rule_supports = np.array(rule_supports).T.astype(float)\n",
    "df_res = pd.DataFrame(\n",
    "    data=list(zip(rule_antecedents, rule_consequents)),\n",
    "    columns=[\"antecedents\", \"consequents\"])\n",
    "\n",
    "sAC = rule_supports[0]\n",
    "sA = rule_supports[1]\n",
    "sC = rule_supports[2]\n",
    "\n",
    "\n",
    "# TODO\n",
    "df_res['total_support'] = 0\n",
    "df_res['antecedent_support'] = 0\n",
    "df_res['consequent_support'] = 0\n",
    "df_res['confidence'] = 0\n",
    "df_res['lift'] = 0\n",
    "df_res['conviction'] = 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
