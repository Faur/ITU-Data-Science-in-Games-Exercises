{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab6 Clustering\n",
    "\n",
    "This week's exercise will focus on clustering using two common algorithms, k-means and k-medoids\n",
    "\n",
    "Schedule:\n",
    "* Implement the core of k-means\n",
    "* Clustering visualizations\n",
    "\n",
    "\n",
    "## Reminders\n",
    "* [GitHub repo](https://github.com/Faur/ITU-Data-Science-in-Games-Exercises)\n",
    "* **Shut down notebooks** when you are done. Otherwise the server will run out of resources, and we will be forced to restart the them.\n",
    "* Server storage is volatile! I.e. you must **save everything locally** that you don't want to loose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes matplotlib plots work better with Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the necessary libraries. \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEFORE YOU START: Classification vs Clustering\n",
    "Most of this excercise, specially the first part, will be very similar to the Lab4 Classification. This is because classification and clustering are similar concepts, but with a very important difference:\n",
    "\n",
    "- Classification assigns labels from a pre-existing set\n",
    "- Clustering groups togheter data by similarity\n",
    "\n",
    "While k-nearest-neighbors returned somethin meaningful like 'GP' (GoalKeeper) and such, clustering algorithms will return something like '', 'Group1' or 'ClusterA'.\n",
    "While after classification the task of the data scientist is to understant is the classification is *correct*, after clustering they need to understand if the clusters are *meaningful*, and what meaning those could have.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the data (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that data and data path is present\n",
    "basedir = \"../\"\n",
    "file = \"fifa.csv\"\n",
    "assert os.path.isdir(f\"{basedir}data\") and os.path.exists(f\"{basedir}data/{file}\"), 'Data not found. Make sure to have the most recent version!'\n",
    "\n",
    "data = pd.read_csv(f'{basedir}/data/fifa.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(data, height=5,hue=\"Position\").map(plt.scatter,\"SprintSpeed\",\"Agility\").add_legend()\n",
    "sns.FacetGrid(data, height=5,hue=\"Position\", col='Preferred Foot').map(plt.scatter,\"ShotPower\",'Penalties').add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plots shows that the `Position` of a player could be related to some of their statistics. The dataset contains 30+ statistics and we don't know which ones will be most helpful, so we are picking an arbitrary subset to avoid the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).\n",
    "It's also possible to choose the best ones [algorithmically](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Feature_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['Crossing','Finishing','HeadingAccuracy','ShortPassing','Volleys','Dribbling','Curve','FKAccuracy','LongPassing',\n",
    "            'BallControl','Acceleration','SprintSpeed','Agility','Reactions','Balance','ShotPower','Jumping','Stamina',\n",
    "            'Strength','LongShots','Aggression','Interceptions','Positioning','Vision','Penalties','Composure',\n",
    "            'Marking','StandingTackle','SlidingTackle','GKDiving','GKHandling','GKKicking','GKPositioning','GKReflexes']\n",
    "\n",
    "features = [\"Aggression\",\"Composure\",\"Penalties\",'SlidingTackle']\n",
    "# cleaning: remove all the lines that contain a NaN in one of the feature columns\n",
    "data = data.dropna(subset=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means\n",
    "\n",
    "0. pick a value for K (number of clusters)\n",
    "1. normalize fields (in our case the data are already normalized)\n",
    "2. create K random centroids as arrays with size the number of features\n",
    "3. foreach `datapoint` in `data`:\n",
    "  1. set as `centroid` of `datapoint` the closest `centroid`\n",
    "  2. if at least one `centroid` has changed: goto 3\n",
    "  \n",
    "### Tips\n",
    "- the computations needed for k-means are expensive, test on a small subset of the data to save time (5/10 to check calculations on single rows, 50/100 to check the correctness of the whole algorithm)\n",
    "- you'll need to compute a distance at a certain point, check Lab4\n",
    "- [DataFrame.idxmin](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.idxmin.html]) will give you the index of the smallest entry in a dataset\n",
    "- `dataFrame1 is dataFrame2` is not correct (it checks if the two objects are the same, not if they contain the same values). The correct approach is `dataFrame1.equals(dataFrame2)` (or `dataFrame1['aColumn'].equals(dataFrame2['aColumn'])` ;))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: define K\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering(data, max_iter=-1):\n",
    "    ## YOUR CODE HERE\n",
    "    centroids = pd.DataFrame(np.random.rand(K, len(features)) * 100, columns=features)\n",
    "\n",
    "    def assign_centroid(x):\n",
    "        x['Centroid'] = centroids.sub(x[features]) \\\n",
    "            .pow(2).sum(1).pow(0.5) \\\n",
    "            .idxmin(0)\n",
    "        return x\n",
    "\n",
    "    iter = 0\n",
    "    data = data.assign(Centroid=pd.Series([-1] * len(data)), OldCentroid=pd.Series([0] * len(data)))\n",
    "    while not data['Centroid'].equals(data['OldCentroid']):\n",
    "        print(f'iter {iter}')\n",
    "        \n",
    "        data['OldCentroid'] = data['Centroid']\n",
    "        data = data.apply(assign_centroid, axis=1)\n",
    "        centroids = data.groupby('Centroid').mean()[features]\n",
    "        centroids.reset_index()\n",
    "        \n",
    "        iter += 1\n",
    "        if max_iter > -1 and iter >= max_iter:\n",
    "            break\n",
    "    \n",
    "    print('done')\n",
    "    return data[features+['Centroid']], centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data, new_centroids = k_means_clustering(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters visualization\n",
    "The most difficult task with clustering is understand if the clustering makes sense and what the clustering means. If the dimensionality is low, a good idea is to use a pairplot and see if the clusters are really close in each slice. The closer they are, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(new_data, vars=features, hue='Centroid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
