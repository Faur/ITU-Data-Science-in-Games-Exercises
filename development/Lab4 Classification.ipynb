{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 Classification and Regression\n",
    "\n",
    "This week's exercise will be a bit shorter. The objective is to implement the core of the k-nn algorithm (most of the pre-processing and evaluation is given), followed by a Q&A moment\n",
    "\n",
    "Schedule:\n",
    "* Classify data using k-neighbors\n",
    "* Use a confusion matrix to evluate models\n",
    "* Q&A on the last lectures\n",
    "\n",
    "## Reminders\n",
    "* [GitHub repo](https://github.com/Faur/ITU-Data-Science-in-Games-Exercises)\n",
    "* **Shut down notebooks** when you are done. Otherwise the server will run out of resources, and we will be forced to restart the them.\n",
    "* Server storage is volatile! I.e. you must **save everything locally** that you don't want to loose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes matplotlib plots work better with Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the necessary libraries. \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that data and data path is present\n",
    "basedir = \"../\"\n",
    "file = \"fifa.csv\"\n",
    "assert os.path.isdir(f\"{basedir}data\") and os.path.exists(f\"{basedir}data/{file}\"), 'Data not found. Make sure to have the most recent version!'\n",
    "\n",
    "data = pd.read_csv(f'{basedir}/data/fifa.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(data, height=5,hue=\"Position\").map(plt.scatter,\"SprintSpeed\",\"Agility\").add_legend()\n",
    "sns.FacetGrid(data, height=5,hue=\"Position\", col='Preferred Foot').map(plt.scatter,\"ShotPower\",'Penalties').add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plots shows that the `Position` of a player could be related to some of their statistics. The dataset contains 30+ statistics and we don't know which ones will be most helpful, so we are picking an arbitrary subset to avoid the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).\n",
    "It's also possible to choose the best ones [algorithmically](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Feature_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['Crossing','Finishing','HeadingAccuracy','ShortPassing','Volleys','Dribbling','Curve','FKAccuracy','LongPassing',\n",
    "            'BallControl','Acceleration','SprintSpeed','Agility','Reactions','Balance','ShotPower','Jumping','Stamina',\n",
    "            'Strength','LongShots','Aggression','Interceptions','Positioning','Vision','Penalties','Composure',\n",
    "            'Marking','StandingTackle','SlidingTackle','GKDiving','GKHandling','GKKicking','GKPositioning','GKReflexes']\n",
    "\n",
    "features = [\"SprintSpeed\",\"Agility\",\"ShotPower\",'Penalties']\n",
    "# cleaning: remove all the lines that contain a NaN in one of the feature columns\n",
    "data = data.dropna(subset=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification problem\n",
    "\n",
    "0. pick a value for K (number of clusters) and N (number of neighbors)\n",
    "1. split the data in train and validation set\n",
    "2. normalize fields (in our case the data are already normalized)\n",
    "3. foreach `datapoint` in `validation set`:\n",
    "  1. find the N nearest neighbors\n",
    "  2. set as label of `datapoint` the label that appears most between its neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: define and N\n",
    "K = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the right K can be tricky. As usual, [stand on the shoulder of giants](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Parameter_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step implemntation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1: let's start with calculating the Euclidean distance from a single datapoint to all the other\n",
    "\n",
    "knn_classification = lambda x, class_col, train_set: (\n",
    "                                train_set[features].sub(x[features])\n",
    "                               .pow(2).sum(1).pow(0.5)\n",
    "                            )\n",
    "\n",
    "# the first row of the result should be 0, since we are calculating the distance between a point and itself\n",
    "# check pandas API for iloc[] if necessary\n",
    "knn_classification(data.iloc[0], 'Position', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.2: to implement k-nn, we need the K closest neighbors. We'll get them with a pandas funciton,\n",
    "# for convenience and performance\n",
    "\n",
    "knn_classification = lambda x, class_col, train_set: (train_set[features].sub(x[features])\n",
    "                               .pow(2).sum(1).pow(0.5)\n",
    "                               .nsmallest(K)\n",
    "                               )\n",
    "\n",
    "knn_classification(data.iloc[0], 'Position', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3: from here, we need the labels associated to this rows. The result is still referencing to our original dataset\n",
    "# (the left column is the index on the original dataset), so we can use iloc[] as before for this\n",
    "\n",
    "# let's also move everything we did up until now in a different function\n",
    "nearest_neighbors = lambda x, train_set: (train_set[features].sub(x[features])\n",
    "                               .pow(2).sum(1).pow(0.5)\n",
    "                               .nsmallest(K)\n",
    "                               ).index # IMPORTANT: .index was missing in the lab (see next cell)\n",
    "\n",
    "\n",
    "knn_classification = lambda x, class_col, train_set: (\n",
    "    # IMPORTANT: .loc[] instead of .iloc[] (see next cell)\n",
    "    train_set.loc[nearest_neighbors(x, train_set)]\n",
    ")\n",
    "\n",
    "\n",
    "knn_classification(data.iloc[0], 'Position', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4: now that we have the neigbors, lets get their labels and select the one that appear more times\n",
    "nearest_neighbors = lambda x, train_set: (train_set[features].sub(x[features])\n",
    "                               .pow(2).sum(1).pow(0.5)\n",
    "                               .nsmallest(K)\n",
    "                               ).index # IMPORTANT: .index was missing in the lab (see next cell)\n",
    "\n",
    "\n",
    "knn_classification = lambda x, class_col, train_set: (\n",
    "    # IMPORTANT: .loc[] instead of .iloc[] (see next cell)\n",
    "    train_set.loc[nearest_neighbors(x, train_set)]\n",
    "        [class_col]\n",
    "        .mode()\n",
    ")\n",
    "\n",
    "# this time we are using a different datapoint, it will show us a problem\n",
    "knn_classification(data.iloc[6], 'Position', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction from the Lab. code\n",
    "\n",
    "During the lab, `iloc[]` has been used insted of `loc[]`. They seems similar but actually they are QUITE different:\n",
    "- `loc[i]` returns the row with index `i`\n",
    "- `iloc[i]` returns the row in the `i`-th position\n",
    "\n",
    "When creating a dataset (unless directly specified), the index of a row is just in position, so both functions will return the same row. That doesn't hold true after manipulating the DataFrame (eg, cleaning, sorting, etc).\n",
    "\n",
    "The correct way to reference a dataset using the entries of a DataFrame `a` from the entries af another DataFrame `b` is:\n",
    "```\n",
    "a.loc[b.index]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5: some of the modes that we will calculate will return more than one value. This won't work,\n",
    "#      so we need to pick one of them. The best solution would be to assign a weight to each neighbor\n",
    "#      based on its distance (see lecture's slides), sum the weights grouped by label and pick the label\n",
    "#      with the highest score. You can try that or ask us for examples. Here we'll use the first entry,\n",
    "#      even if this will degrade our performance (possubly by quite a lot)\n",
    "\n",
    "nearest_neighbors = lambda x, train_set: (train_set[features].sub(x[features])\n",
    "                               .pow(2).sum(1).pow(0.5)\n",
    "                               .nsmallest(K)\n",
    "                               ).index # IMPORTANT: .index was missing in the lab (see next cell)\n",
    "\n",
    "\n",
    "knn_classification = lambda x, class_col, train_set: (\n",
    "    # IMPORTANT: .loc[] instead of .iloc[] (see next cell)\n",
    "    train_set.loc[nearest_neighbors(x, train_set)]\n",
    "        [class_col]\n",
    "        .mode()[0]\n",
    ")\n",
    "\n",
    "\n",
    "knn_classification(data.iloc[6], 'Position', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(dataset, class_col, random_state=None, frac=0.995):\n",
    "    # 1: random split. It's always a good idea (maybe the dataset is sorted, and so on).\n",
    "    # random_state that you to always get the same split (useful for testing). Check docs\n",
    "    train_set = data.sample(frac=frac, random_state=random_state)\n",
    "    valid_set = data.drop(train_set.index)\n",
    "\n",
    "    classified_set = valid_set\n",
    "    classified_set[f'Calculated {class_col}'] = valid_set.apply(\n",
    "        lambda x: knn_classification(x, class_col, train_set),\n",
    "        axis=1\n",
    "    )\n",
    "    return classified_set\n",
    "\n",
    "classified_position = classify(data, 'Position')[['Position', 'Calculated Position']]\n",
    "classified_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification with positive class\n",
    "In many cases, it's important to determine if a datapoint belongs or not to a certain class or not and the distribution is skewed (poisonus/not poisonus, cancer/not cancer, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "\n",
    "data['Goalkeeper'] = data['Position'] == 'GK'\n",
    "\n",
    "classified_goalkeeper = classify(data, 'Goalkeeper', frac=0.99)[['Goalkeeper', 'Calculated Goalkeeper']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification evaluation\n",
    "\n",
    "1. generate confusion matrix\n",
    "2. evaluate the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_results(data, class_col, binary=False):\n",
    "    labels = data[class_col].unique()\n",
    "    cm = confusion_matrix(data[class_col], data[f'Calculated {class_col}'], labels=labels)\n",
    "\n",
    "    print_confusion_matrix(\n",
    "        cm,\n",
    "        labels\n",
    "    )\n",
    "    plt.show()  # wait for the render (otherwise it will print the graphs async, after the strings)\n",
    "    \n",
    "    if binary:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        accuracy = (tp + tn) / (tn + fp + fn + tp)\n",
    "        err_rate = (fp + fn) / (tn + fp + fn + tp)\n",
    "        sensitiv = (tp) / (fp + tp)\n",
    "        specific = (tn) / (tn + fn)\n",
    "        precision = (tp) / (tp + tn)\n",
    "\n",
    "        print(cm.ravel())\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        print(f'Error rate: {err_rate}')\n",
    "        print(f'Sensitivity: {sensitiv}')\n",
    "        print(f'Specificity: {specific}')\n",
    "        print(f'Precision: {specific}')\n",
    "    else:\n",
    "        diag = [cm[i][i] for i in range(len(cm))]        \n",
    "        rows = [sum(cm[i]) for i in range(len(cm))]\n",
    "        \n",
    "        right_pred = sum(diag)\n",
    "        wrong_pred = sum(rows)\n",
    "        \n",
    "        print(f'Precision: {right_pred / wrong_pred}')\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "print_results(classified_position, 'Position')\n",
    "print_results(classified_goalkeeper, 'Goalkeeper', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy seems high for the `Goalkeeper` class, but because the percentage of goalkeepers is low. In those cases, Snsitivity is a much better metric. Here will probably be very low because the metric are chosen arbitrarly. You can check the link above to see how to pick appropriate dimensions.\n",
    "\n",
    "Additionally, for skewed distributions:\n",
    "> A drawback of the basic \"majority voting\" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the k nearest neighbors due to their large number[[4]](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#cite_note-Coomans_Massart1982-4). One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its k nearest neighbors. The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
