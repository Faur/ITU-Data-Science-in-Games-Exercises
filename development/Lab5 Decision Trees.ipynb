{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "Tree-based models work by partitioning the input space into rectangular regions, and then assigning simple (typically constant) model to each region.\n",
    "\n",
    "Tree-based models: Pro/Con\n",
    "* `+` simple\n",
    "* `+` powerful\n",
    "* `+` human understandable\n",
    "* `-` computationally expensive\n",
    "* `-` The optimal tree cannot be found efficiently. Therefore trees are built using a heuristic.\n",
    "\n",
    "In this notebook we will see how to make a simple decision tree.\n",
    "There are a couple of different kinds, but we will focus on one called CART (Classification and Regression Trees).\n",
    "CART is a nice to learn with, as it is easy to understand and implement.\n",
    "\n",
    "In this notebook we focus on **classification** on the Iris dataset, a classical data science dataset.\n",
    "The goal is to predict the type of iris plant ('setosa' 'versicolor' 'virginica') from sepal and petal dimensions.\n",
    "With only relatively minor changes the decision tree we make here could also be used for regression as well.\n",
    "\n",
    "![](https://www.wpclipart.com/plants/diagrams/plant_parts/petal_sepal_label.png)\n",
    "\n",
    "\n",
    "## Reminders\n",
    "* [GitHub repo](https://github.com/Faur/ITU-Data-Science-in-Games-Exercises)\n",
    "* **Shut down notebooks** when you are done. Otherwise the server will run out of resources, and we will be forced to restart the them.\n",
    "* Server storage is volatile! I.e. you must **save everything locally** that you don't want to loose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART\n",
    "A CART is a binary tree.\n",
    "* Inner nodes (non-leaf nodes) have a splitting rule, which can be described as `if-then` rules.\n",
    "If some condition is meet go to the left child, otherwise go to the right child.\n",
    "     * Each inner node queries one of the features, and checks whether it fulfils some condition, typically a threshold or equality.\n",
    "     * If the condition is meet go to left child, otherwise go right child.\n",
    "* Continue like this until a leaf node is reached. Once a leaf node is reached assign $x$ to the corresponding label or value.\n",
    "\n",
    "\n",
    "The splitting rule should, at each node split the data, $S$, into $L_{d,\\theta}$ (left) and $R_{d,\\theta}$ (right) such that the information gain is maximized:\n",
    "$$\n",
    "G_{d,\\theta}(S) = Q(S) - \\frac{|L_{d,\\theta}|}{|S|}Q(L_{d,\\theta}) - \\frac{|R_{d,\\theta}|}{|S|}Q(R_{d,\\theta})\n",
    "$$\n",
    "Where $Q$ is some impurity measure.\n",
    "Until $|S|$ is smaller than some $s_{threshold}$ or all elements belong to the same class.\n",
    "Once the tree is grown it is common to prune to reduce its complexity - but we won't do that in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "For this exercise we will combine the features and targets in one array.\n",
    "This makes keeping track of the data a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# print(iris.DESCR)  # <-- Uncomment for more details on the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target Names:  ['setosa' 'versicolor' 'virginica']\n",
      "data\t\t (150, 5)\n",
      "data_test\t (15, 5)\n",
      "data_train\t (135, 5)\n",
      "\n",
      "Example data\n",
      "[[4.9 3.  1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]]\n"
     ]
    }
   ],
   "source": [
    "data = np.concatenate([iris.data, iris.target[:,None]], axis=-1) \n",
    "\n",
    "step_len = 10\n",
    "data_test = data[::step_len,:]\n",
    "np.random.shuffle(data_test)\n",
    "data_train = np.delete(data, [step_len*i for i in range(data_test.shape[0])], axis=0)#.reshape([-1, data.shape[1]])\n",
    "\n",
    "print('Feature Names:', iris.feature_names)\n",
    "print('Target Names: ', iris.target_names)\n",
    "print('data\\t\\t', data.shape)\n",
    "print('data_test\\t', data_test.shape)\n",
    "print('data_train\\t', data_train.shape)\n",
    "print()\n",
    "\n",
    "print('Example data')\n",
    "print(data_train[::15,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful helper functions.\n",
    "# Reading and understanding these is a good idea!\n",
    "\n",
    "def class_counts(data):\n",
    "    \"\"\"Counts the number of each class label in the input dataset.\"\"\"\n",
    "    counts = {}  # a dictionary of label -> count.\n",
    "    for row in data:\n",
    "        # in our dataset format, the label is always the last column\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "def is_numeric(value):\n",
    "    \"\"\"Test if a value is numeric.\"\"\"\n",
    "    return isinstance(value, int) or isinstance(value, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class distribution is as follows:\n",
      "Total:\t {2.0: 50, 0.0: 50, 1.0: 50}\n",
      "Train:\t {0.0: 45, 1.0: 45, 2.0: 45}\n",
      "Test:\t {2.0: 5, 1.0: 5, 0.0: 5}\n"
     ]
    }
   ],
   "source": [
    "print('The class distribution is as follows:')\n",
    "print('Total:\\t', class_counts(data))\n",
    "print('Train:\\t', class_counts(data_train))\n",
    "print('Test:\\t', class_counts(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impurity Measure\n",
    "\n",
    "The first thing we need to do is to define an impurity measure.\n",
    "There are a couple of different to choose between\n",
    " * https://en.wikipedia.org/wiki/Decision_tree_learning#Metrics\n",
    "\n",
    "For this exercise we will use the Gini impurity (see Wikipedia).\n",
    "The Gini impurity is the probability of picking two different objects when drawing two random samples from the poulation.\n",
    "\n",
    "$$\n",
    "I_G(p) = 1 - \\sum_{i=1} p_i^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_impurity(data_class)\t\t 0.6666666666666665\n",
      "compute_impurity(data_class[:90])\t 0.5\n",
      "compute_impurity(data_class[:45])\t 0.0\n"
     ]
    }
   ],
   "source": [
    "def compute_impurity(data):\n",
    "    \"\"\" Calculate the impurity using the Gini Impurity Metric.\n",
    "        \n",
    "        Write the function generally, such that it works with different numbers of classes.\n",
    "    \"\"\"\n",
    "    # determine number of classes in data (last column)\n",
    "    label_counts = class_counts(data)\n",
    "    n = len(data)\n",
    "    impurity = 1\n",
    "    for label in label_counts:\n",
    "        prob_of_label = label_counts[label] / n\n",
    "        impurity -= prob_of_label**2\n",
    "    return impurity\n",
    "\n",
    "print('compute_impurity(data_class)\\t\\t', compute_impurity(data_train))\n",
    "print('compute_impurity(data_class[:90])\\t', compute_impurity(data_train[:90]))\n",
    "print('compute_impurity(data_class[:45])\\t', compute_impurity(data_train[:45]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Branching\n",
    "> Estimated time: 30 minutes.\n",
    "\n",
    "The next thing we need to be able to do is use the impurity measure to split the data.\n",
    "A split is based on a `if-then` rule, here called a `Question`.\n",
    "A question is associated with a coordinate $d\\in \\{1, ..., D\\}$ (what feature are we asking about), and a threshold, $\\theta$.\n",
    "* E.g: if $x_d < \\theta$ go to left child, otherwise go to right child.\n",
    "\n",
    "Note that each feature is considered independently - therefore decision trees aren't affected by normalization.\n",
    "\n",
    "Your task is to:\n",
    "1. Finish the `match` method in the `Question` class. \n",
    " * Be sure that it can handle both categorican and numerical data (even though we only use numerical data here).\n",
    "1. Complete the `partition` function in the next cell.\n",
    " * Use `compute_impurity` and `Question` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\" A Question is used to partition a dataset.\n",
    "        This class just records a 'column number' (aka coordinate) and a\n",
    "        'column value' (aka threshold). The 'match' method is used to compare\n",
    "        the feature value in an example to the feature value stored in the\n",
    "        question. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "        self.value_is_numerical = is_numeric(self.value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Helper method to print in a readable format.\n",
    "        condition = \"==\"\n",
    "        if self.value_is_numerical:\n",
    "            condition = \">=\"\n",
    "        return \"Is %s %s %s?\" % (\n",
    "            iris.feature_names[self.column], condition, str(self.value))\n",
    "        \n",
    "    def match(self, example):\n",
    "        \"\"\" Takes a single example (a single row) and compares \n",
    "            it with the feature value in this question.\n",
    "            \n",
    "            Returns: Bool\n",
    "        \"\"\"\n",
    "        pass ## YOUR CODE HERE\n",
    "        val = example[self.column]\n",
    "        if self.value_is_numerical:\n",
    "            return self.value < val \n",
    "        else:\n",
    "            return self.value == val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(rows, question):\n",
    "    \"\"\" Partitions a dataset:\n",
    "        For each row in the dataset, check if it matches the question. \n",
    "        If so, add it to 'true rows', otherwise, add it to 'false rows'.\n",
    "        \n",
    "        Returns: true_rows, false_rows\n",
    "    \"\"\"\n",
    "    \n",
    "    true_rows, false_rows = [], []\n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "    for row in rows:\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "    return true_rows, false_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Best Split\n",
    "\n",
    "> Estimated time: 30 minutes.\n",
    "\n",
    "Now that we know how to split we need to determine where to split!\n",
    "The best split is the one that maximizes the **information gain**, as determiend by the impurity measure.\n",
    "\n",
    "$$\n",
    "G_{d,\\theta}(S) = Q(S) - \\frac{|L_{d,\\theta}|}{|S|}Q(L_{d,\\theta}) - \\frac{|R_{d,\\theta}|}{|S|}Q(R_{d,\\theta})\n",
    "$$\n",
    "\n",
    "\n",
    "Your task is to:\n",
    "1. Complete the `find_best_split` function. \n",
    "\n",
    " * You will need to combine several things: `compute_impurity`, `Question`, `partition`, and `info_gain`.\n",
    " * Hint: Structure your code as a double for-loop such that you examine whether each value for each feature is a good threshold.\n",
    "    * Loop through all the features \n",
    "    * For each value check if this is a good `Question`.\n",
    "    * Remember to incorporate the `minimum_size` parameter.\n",
    "    * Return the best information gain, and the question that lead to it. (or `0, None` if no good questions exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(left, right, current_uncertainty):\n",
    "    \"\"\" Information Gain.\n",
    "        The uncertainty of the starting node, minus the weighted impurity of two child nodes.\n",
    "    \"\"\"\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_uncertainty - p * compute_impurity(left) - (1 - p) * compute_impurity(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(rows, minimum_size=10):\n",
    "    \"\"\" Find the best split by finding the information gain for each possible split.\n",
    "    \n",
    "        Returns: best_gain, best_question\n",
    "    \"\"\"\n",
    "    best_gain = 0  # keep track of the best information gain\n",
    "    best_question = None  # keep track of the feature / value that produced it\n",
    "    current_uncertainty = compute_impurity(rows)\n",
    "    n_features = len(rows[0]) - 1  # number of columns\n",
    "\n",
    "    ## YOUR CODE HERE\n",
    "    for col in range(n_features):  # for each feature        \n",
    "        values = set([row[col] for row in rows])  # unique values in the column\n",
    "        for val in values:  # for each value\n",
    "\n",
    "            question = Question(col, val)\n",
    "            # try splitting the dataset\n",
    "            true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "            # Skip this split if it doesn't divide the dataset.\n",
    "            if len(true_rows) < minimum_size or len(false_rows) < minimum_size:\n",
    "                continue\n",
    "\n",
    "            # Calculate the information gain from this split\n",
    "            gain = info_gain(true_rows, false_rows, current_uncertainty)\n",
    "\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_question = gain, question\n",
    "\n",
    "    return best_gain, best_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Growing Trees\n",
    "\n",
    "Now we have all the ingredients to start growing trees.\n",
    "Have a look, and see if that works.\n",
    "\n",
    "\n",
    "`build_tree` function is implemented using **recursion**.\n",
    "Each iteration checks whether we shuold create a `Leaf_Node` (the base case) or it partitions the input date, and recursively calls itself on both partitions.\n",
    "The result of the recursive function is saved as a `Decision_Node`.\n",
    "\n",
    "\n",
    "`classify` is als implemented with recursion function.\n",
    "The function takes **one** observation, and return the predicted class, by at each `Decision_Node` asking the `Question`.\n",
    "Based on the answer follow the true-branch or the false-branch until you hit a `Leaf_Node`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf_Node:\n",
    "    \"\"\" A Leaf_Node classifies data.\n",
    "        This holds a dictionary of class (e.g., \"Apple\") -> number of times\n",
    "        it appears in the rows from the training data that reach this leaf.\n",
    "    \"\"\"\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = class_counts(rows)\n",
    "\n",
    "class Decision_Node:\n",
    "    \"\"\" A Decision Node asks a question.\n",
    "        This holds a reference to the question, and to the two child nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 true_branch,\n",
    "                 false_branch):\n",
    "        \n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(rows):\n",
    "    \"\"\" Builds the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the best split by finding the information gain for each possible split\n",
    "    gain, question = find_best_split(rows)\n",
    "\n",
    "    # Base case: no further info gain\n",
    "    # Since we can ask no further questions,\n",
    "    # we'll return a leaf.\n",
    "    if gain == 0:\n",
    "        return Leaf_Node(rows)\n",
    "\n",
    "    # If we reach here, we have found a useful feature / value\n",
    "    # to partition on.\n",
    "    true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "    # Recursively build the true branch.\n",
    "    true_branch = build_tree(true_rows)\n",
    "\n",
    "    # Recursively build the false branch.\n",
    "    false_branch = build_tree(false_rows)\n",
    "\n",
    "    # Return a Question node.\n",
    "    # This records the best feature / value to ask at this point,\n",
    "    # as well as the branches to follow\n",
    "    # dependingo on the answer.\n",
    "    return Decision_Node(question, true_branch, false_branch)\n",
    "\n",
    "my_tree = build_tree(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(row, node, verbose=False):\n",
    "    \"\"\" Recursively search\n",
    "    \"\"\"\n",
    "    \n",
    "    pass ## YOUR CODE HERE\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf_Node):\n",
    "        return node.predictions\n",
    "\n",
    "    # Decide whether to follow the true-branch or the false-branch.\n",
    "    # Compare the feature / value stored in the node,\n",
    "    # to the example we're considering.\n",
    "    if verbose:\n",
    "        print(node.question, node.question.match(row))\n",
    "\n",
    "    if node.question.match(row):\n",
    "        return classify(row, node.true_branch, verbose)\n",
    "    else:\n",
    "        return classify(row, node.false_branch, verbose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Results\n",
    "\n",
    "Now comes the fun part!\n",
    "Run the cells below, and interpret the results.\n",
    "\n",
    "Note that the Iris dataset is a quite simple dataset, and you shouldn't expect to get nearly this good results in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is petal width (cm) >= 0.6?\n",
      "--> True:\n",
      "|.Is petal width (cm) >= 1.7?\n",
      "|.--> True:\n",
      "|.|.Predict {2.0: 40}\n",
      "|.--> False:\n",
      "|.|.Is petal length (cm) >= 4.6?\n",
      "|.|.--> True:\n",
      "|.|.|.Predict {1.0: 9, 2.0: 4}\n",
      "|.|.--> False:\n",
      "|.|.|.Is sepal width (cm) >= 2.5?\n",
      "|.|.|.--> True:\n",
      "|.|.|.|.Predict {1.0: 26}\n",
      "|.|.|.--> False:\n",
      "|.|.|.|.Predict {1.0: 10, 2.0: 1}\n",
      "--> False:\n",
      "|.Predict {0.0: 45}\n"
     ]
    }
   ],
   "source": [
    "def print_tree(node, spacing=\"\"):\n",
    "    \"\"\" World's most elegant tree printing function.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf_Node):\n",
    "        print (spacing + \"Predict\", node.predictions)\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    print (spacing + str(node.question))\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    print (spacing + '--> True:')\n",
    "    print_tree(node.true_branch, spacing + \"|.\")\n",
    "\n",
    "    # Call this function recursively on the false branch\n",
    "    print (spacing + '--> False:')\n",
    "    print_tree(node.false_branch, spacing + \"|.\")\n",
    "\n",
    "print_tree(my_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? True\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? False\n",
      "Is petal length (cm) >= 4.6? True\n",
      "Actual: 1.0. Predicted: {1.0: '69%', 2.0: '30%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? False\n",
      "Is petal length (cm) >= 4.6? False\n",
      "Is sepal width (cm) >= 2.5? False\n",
      "Actual: 1.0. Predicted: {1.0: '90%', 2.0: '9%'}\n",
      "\n",
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "def print_leaf(counts):\n",
    "    \"\"\"A nicer way to print the predictions at a leaf.\"\"\"\n",
    "    total = sum(counts.values()) * 1.0\n",
    "    probs = {}\n",
    "    for lbl in counts.keys():\n",
    "        probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
    "    return probs\n",
    "\n",
    "acc = 0\n",
    "for row in data_test:\n",
    "    if acc >= 3:\n",
    "        pred = classify(row, my_tree, False)\n",
    "    else:\n",
    "        pred = classify(row, my_tree, True)\n",
    "        print (\"Actual: %s. Predicted: %s\" % (row[-1], print_leaf(pred)))\n",
    "        print()\n",
    "\n",
    "    if row[-1] in pred:\n",
    "        acc += 1\n",
    "\n",
    "print('Accuracy:', acc/data_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook is made heavily inspired and borrowing from: [Google Developers: Machine Learning Recipes #8](https://www.youtube.com/watch?v=LDRbO9a6XPU) ([source code](https://github.com/random-forests/tutorials/)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
