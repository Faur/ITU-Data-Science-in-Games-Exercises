{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1 Python for data science!\n",
    "\n",
    "This is an introductory class\n",
    "\n",
    "The objectives of this lab is for you to get:\n",
    "* familiar with the lab strucutre\n",
    "* a Python crash course / reminder\n",
    "* an introduction / reminder to key data science libraries (Numpy, Pandas)\n",
    "* experience with loading, validating, and visualizing data.\n",
    "\n",
    "## Quick note on the labs\n",
    "The labs will be made available on [GitHub](https://github.com/Faur/ITU-Data-Science-in-Games-Exercises) on a roling basis.\n",
    "Be sure to have the most recent version locally by pulling from the repo.\n",
    "This can be done from the notebook by using the cell below.\n",
    "Remove the comment symbol `#` and run the cell (`Ctrl` + `Enter`).\n",
    "`!` tells the notebook to run the command in the terminal, instead of in thr Python interpriter.\n",
    "\n",
    "Some important notes:\n",
    "* **Shut down notebooks** when you are done. Otherwise the server will run out of resources, and we will be forced to restart the them.\n",
    "* Server storage is volatile! I.e. you must **save everything locally** that you don't want to loose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Crash Course\n",
    "\n",
    "If you are new to Python [this 45 min video](https://www.youtube.com/watch?v=N4mEzFDjqtA) gives a good introduction to the key concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes matplotlib plots work better with Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the necessary libraries. \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Loading the data\n",
    "\n",
    "> Estimated task time: 10 minutes.\n",
    "\n",
    "The first this first assignment you must \n",
    "\n",
    "1) load the data in `./data/Data-Mining-Spring-2018.csv` using **`pandas`**.\n",
    "Pandas is an open-source Python library providing high-performance, easy-to-use data structures and data analysis tools.\n",
    "It is very popular among data scientists and statisticians as it allows you to work very quickly and efficiently.\n",
    " * Have a look at the `pandas.read_csv` function.\n",
    "\n",
    "2) Make sense of the data by printing the first 10 values. Determine the number of observations and features in the data, and have a quick look at what data types they are (or should be).\n",
    "* Pandas dataframes have the `head` method that is useful printing a limited number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that data and data path is present\n",
    "basedir = \"./\"\n",
    "assert os.path.isdir(basedir+\"data\") and os.path.exists(basedir + \"data/Data-Mining-Spring-2018.csv\"), 'Data not found. Make sure to have the most recent version!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE \n",
    "data = pd.read_csv(basedir+'data/Data-Mining-Spring-2018.csv', sep=\",\")\n",
    "print('Data shape:', data.shape)\n",
    "print('I.e. there are',data.shape[0],'observations, each with',data.shape[1],'features.')\n",
    "# print(data.columns.values)  # <-- Print the name of all the feature.\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Cleaning the data\n",
    "\n",
    "> Estimated task time: 10 minutes.\n",
    "\n",
    "We don't want to work with all the features for this exercise.\n",
    "\n",
    "1) Select a subset of the features, as defined by `feature_sub` (cell below).\n",
    "\n",
    "2) Rename the columns, such that `What degree are you studying?` becomes `Degree`, and `Shoe Size` becomes `ShoeSize`. Not having spaces (or long names) makes it easier to work with the data in `pandas`.\n",
    " * Look at the `rename` method.\n",
    "\n",
    "3) Convert the columns to the appropriate data formats (e.g. `Age` should be a float, and `Gender` should be a string).\n",
    " * `to_numeric` is a useful method (but not the only way) to convert strings to numerical values, and the `errors` argument can be used to handle errors.\n",
    " * `dropna` can be used to remove `nan` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sub = ['Age', 'Gender', 'Shoe Size', 'Height', 'What degree are you studying?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "data_sub = data[feature_sub]\n",
    "\n",
    "\n",
    "data_sub = data_sub.rename(columns={'What degree are you studying?': \"Degree\"})\n",
    "data_sub = data_sub.rename(columns={'Shoe Size': \"ShoeSize\"})\n",
    "\n",
    "\n",
    "data_sub['Age'] = pd.to_numeric(data_sub['Age'], errors='coerce')\n",
    "data_sub['ShoeSize'] = pd.to_numeric(data_sub['ShoeSize'], errors='coerce')\n",
    "data_sub['Height'] = pd.to_numeric(data_sub['Height'], errors='coerce')\n",
    "\n",
    "data_sub['Gender'] = data_sub['Gender'].apply(str)\n",
    "data_sub['Degree'] = data_sub['Degree'].apply(str)\n",
    "data_sub = data_sub.dropna()\n",
    "\n",
    "print(data_sub.describe())\n",
    "print('\\n\\n')\n",
    "print(data_sub.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "Now that we have `Age`, `ShoeSize`, and `Height` as numerical we can start visualizing it.\n",
    "Simple visualizations, like histograms are an easy way to get a sense of the data, check for outliers, faulty or anything else we need to take care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot(data):\n",
    "    plt.figure(figsize=[8,8])\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.hist(data.Age.values)\n",
    "    plt.title(\"Age\")\n",
    "\n",
    "    plt.subplot(223)\n",
    "    plt.hist(data.ShoeSize.values)\n",
    "    plt.title(\"ShoeSize\")\n",
    "\n",
    "    plt.subplot(224)\n",
    "    plt.hist(data.Height.values)\n",
    "    plt.title(\"Height\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "hist_plot(data_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Remove Invalid values\n",
    "\n",
    "> Estimated task time: 15 minutes.\n",
    "\n",
    "In the histograms above we see that several values seem suspicious, e.g. a height of 19 cm is probably not true.\n",
    "In this exercise your job is to remove the faulty observations.\n",
    "This is ofcourse fundamentally a subjective taks, where you will have to rely on your domain knowledge.\n",
    "\n",
    "1) Remove the observations with invalid data points.\n",
    " * `df.where`/`df.mask` in conjunction with `dropna` can be useful for these kinds of operations.\n",
    "\n",
    "2) Visualize the data again. If it still looks strange go back to 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "data_sub.mask(data_sub.Age < 15, np.nan, inplace=True)\n",
    "data_sub.mask(data_sub.Age > 70, np.nan, inplace=True)\n",
    "\n",
    "data_sub.mask(data_sub.ShoeSize < 20, np.nan, inplace=True)\n",
    "data_sub.mask(data_sub.ShoeSize > 60, np.nan, inplace=True)\n",
    "\n",
    "data_sub.mask(data_sub.Height < 100, np.nan, inplace=True)\n",
    "data_sub.mask(data_sub.Height > 250, np.nan, inplace=True)\n",
    "\n",
    "data_sub = data_sub.dropna()\n",
    "hist_plot(data_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Convert Gender to Integers\n",
    "\n",
    "> Estimated task time: 15 minutes.\n",
    "\n",
    "We often prefer working with integer class labels, rather than strings. \n",
    "As you can see the gender has been specified in several different ways, so you need to do some work making the data interpritable.\n",
    "For this taks you should:\n",
    "\n",
    "1) Create a new column called `GenderNumerical` with 0's for males, 1's for females, and 2's for other.\n",
    " * Define a function that interprets the `Gender` string, and returns the appropriate number.\n",
    "  * Python distinguishes between upper and lower case, so when working with strings it can sometimes help converting everything to lower case.\n",
    " * Use `df.apply` to apply the function to every element in the dataframe.\n",
    "\n",
    "2) Determine the ratio of the three gender categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Values in the 'Gender' column:\")\n",
    "print(np.unique(data_sub['Gender']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_gender_numerical(string):\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    string = string.lower()\n",
    "    if string in ['man', 'male', 'alpha male', 'man ', 'm']:\n",
    "        return 0\n",
    "    elif string in ['f', 'female', 'woman']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "data_sub['GenderNumerical'] = data_sub['Gender'].apply(determine_gender_numerical)\n",
    "\n",
    "print(\"Male\\t\", np.sum(data_sub.GenderNumerical==0) / len(data_sub))\n",
    "print(\"Female\\t\", np.sum(data_sub.GenderNumerical==1) / len(data_sub))\n",
    "print(\"Other\\t\", np.sum(data_sub.GenderNumerical==2) / len(data_sub))\n",
    "\n",
    "data_sub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot visualization\n",
    "\n",
    "Now that we removed the faulty observations in the data we can visualize it further.\n",
    "As long as the number of features is small a pair plot is an easy way to quickly get an overview of relationships between the different features.\n",
    "\n",
    "We can make such a plot easily using `seaborn`, a popular visualization library.\n",
    "It is based on `matplotlib`, but provides a higher-level API, making it one of the easiest ways to make pretty plots.\n",
    "It also has nice `pandas` integration, as shown below.\n",
    "Another cool python library to look into is [`bokeh`](https://bokeh.pydata.org).\n",
    "It allows you to easily create interactive plots.\n",
    "\n",
    "**Question**: What relationships do you see in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data_sub, hue=\"Degree\", diag_kind='hist')\n",
    "# diag_kind='hist' is necessary when you have small classes, as kde-plot fails for classes with one observation.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Normalize Data\n",
    "\n",
    "> Estimated task time: 10 minutes.\n",
    "\n",
    "For the last task you must normalize the data.\n",
    "Many data science methods require that we first normalize the data.\n",
    "Typically we would want to use a library (e.g. `sklearn.preprocessing.normalize`), but for this task you should do it yourself.\n",
    "\n",
    "\n",
    "1) Make a new DataFrame, `data_norm`, where all the floating point columns are normalized using to zero mean and unit (1) variance using the following equation:\n",
    "$$\n",
    "x_{norm} = \\frac{x-\\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "2) Add the `Degree` column to the normalized DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm = None ## YOUR CODE HERE\n",
    "\n",
    "def normalize(df):\n",
    "    result = pd.DataFrame()\n",
    "    for feature_name in df.columns:\n",
    "        if df[feature_name].dtype == np.float64 or df[feature_name].dtype == np.int64:\n",
    "            result[feature_name] = (df[feature_name] - df[feature_name].mean()) / (df[feature_name].std())\n",
    "    return result\n",
    "\n",
    "data_norm = normalize(data_sub)\n",
    "data_norm['Degree'] = data_sub.Degree\n",
    "\n",
    "data_norm.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Visualization\n",
    "\n",
    "One example of a method that requires normalization is Principal Component Analsysis (PCA).\n",
    "PCA is a popular visualization technique, as it allows you to project high dimensional data into a low dimensional space.\n",
    "This is usefull for reducing the number of features, or for visualizing features.\n",
    "\n",
    "A simple PCA plot that projects 'Age', 'ShoeSize', 'GenderNumerical' down into 2 dimensions is performed below.\n",
    "If we color the dots with the remaining two attributes 'Degree' and 'Height' we see that PCA doesn't seem to separate 'Degree', where as 'Height' is separated somewhat nicely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data_as_numpy = data_norm[['Age', 'ShoeSize', 'GenderNumerical']].values\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(data_as_numpy)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2'])\n",
    "\n",
    "principalDf['Degree'] = data_sub.Degree.values\n",
    "principalDf['Height'] = data_sub.Height.values\n",
    "\n",
    "sns.scatterplot(data=principalDf, x='PC1', y='PC2', hue='Degree')\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(data=principalDf, x='PC1', y='PC2', hue='Height', palette='RdBu')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
