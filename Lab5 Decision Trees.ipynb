{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "Tree-based models work by partitioning the input space into rectangular regions, and then assigning simple (typically constant) model to each region.\n",
    "\n",
    "Tree-based models: Pro/Con\n",
    "* `+` simple\n",
    "* `+` powerful\n",
    "* `+` human understandable\n",
    "* `-` computationally expensive\n",
    "* `-` The optimal tree cannot be found efficiently. Therefore trees are built using a heuristic.\n",
    "\n",
    "In this notebook we will see how to make a simple decision tree.\n",
    "There are a couple of different kinds, but we will focus on one called CART (Classification and Regression Trees).\n",
    "CART is a nice to learn with, as it is easy to understand and implement.\n",
    "\n",
    "In this notebook we focus on **classification** on the Iris dataset, a classical data science dataset.\n",
    "The goal is to predict the type of iris plant ('setosa' 'versicolor' 'virginica') from sepal and petal dimensions.\n",
    "With only relatively minor changes the decision tree we make here could also be used for regression as well.\n",
    "\n",
    "![](https://www.wpclipart.com/plants/diagrams/plant_parts/petal_sepal_label.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART\n",
    "A CART is a binary tree.\n",
    "* Inner nodes (non-leaf nodes) have a splitting rule, which can be described as `if-then` rules.\n",
    "If some condition is meet go to the left child, otherwise go to the right child.\n",
    "     * Each inner node queries one of the features, and checks whether it fulfils some condition, typically a threshold or equality.\n",
    "     * If the condition is meet go to left child, otherwise go right child.\n",
    "* Continue like this until a leaf node is reached. Once a leaf node is reached assign $x$ to the corresponding label or value.\n",
    "\n",
    "\n",
    "The splitting rule should, at each node split the data, $S$, into $L_{d,\\theta}$ (left) and $R_{d,\\theta}$ (right) such that the information gain is maximized:\n",
    "$$\n",
    "G_{d,\\theta}(S) = Q(S) - \\frac{|L_{d,\\theta}|}{|S|}Q(L_{d,\\theta}) - \\frac{|R_{d,\\theta}|}{|S|}Q(R_{d,\\theta})\n",
    "$$\n",
    "Where $Q$ is some impurity measure.\n",
    "Until $|S|$ is smaller than some $s_{threshold}$ or all elements belong to the same class.\n",
    "Once the tree is grown it is common to prune to reduce its complexity - but we won't do that in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "For this exercise we will combine the features and targets in one array.\n",
    "This makes keeping track of the data a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# print(iris.DESCR)  # <-- Uncomment for more details on the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 5)\n",
      "(135, 5)\n",
      "Feature Names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target Names:  ['setosa' 'versicolor' 'virginica']\n",
      "data\t\t (150, 5)\n",
      "data_test\t (15, 5)\n",
      "data_train\t (135, 5)\n"
     ]
    }
   ],
   "source": [
    "data = np.concatenate([iris.data, iris.target[:,None]], axis=-1)\n",
    "\n",
    "step_len = 10\n",
    "data_test = data[::step_len,:]\n",
    "np.random.shuffle(data_test)\n",
    "data_train = np.delete(data, [step_len*i for i in range(data_test.shape[0])], axis=0)#.reshape([-1, data.shape[1]])\n",
    "\n",
    "print(data_test.shape)\n",
    "print(data_train.shape)\n",
    "\n",
    "print('Feature Names:', iris.feature_names)\n",
    "print('Target Names: ', iris.target_names)\n",
    "print('data\\t\\t', data.shape)\n",
    "print('data_test\\t', data_test.shape)\n",
    "print('data_train\\t', data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful helper functions.\n",
    "# Reading and understanding these is a good idea!\n",
    "\n",
    "def class_counts(data):\n",
    "    \"\"\"Counts the number of each class label in the input dataset.\"\"\"\n",
    "    counts = {}  # a dictionary of label -> count.\n",
    "    for row in data:\n",
    "        # in our dataset format, the label is always the last column\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "def is_numeric(value):\n",
    "    \"\"\"Test if a value is numeric.\"\"\"\n",
    "    return isinstance(value, int) or isinstance(value, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class distribution is as follows:\n",
      "Total:\t {2.0: 50, 0.0: 50, 1.0: 50}\n",
      "Train:\t {0.0: 45, 1.0: 45, 2.0: 45}\n",
      "Test:\t {2.0: 5, 1.0: 5, 0.0: 5}\n"
     ]
    }
   ],
   "source": [
    "print('The class distribution is as follows:')\n",
    "print('Total:\\t', class_counts(data))\n",
    "print('Train:\\t', class_counts(data_train))\n",
    "print('Test:\\t', class_counts(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Impurity Measure\n",
    "\n",
    "> Estimated time: 10 minutes.\n",
    "\n",
    "The first thing we need to do is to define an impurity measure.\n",
    "There are a couple of differnet to choose between\n",
    " * https://en.wikipedia.org/wiki/Decision_tree_learning#Metrics\n",
    "\n",
    "For this exercise we will use the Gini impurity (see Wikipedia).\n",
    "The Gini impurity is the probability of picking two different objects when drawing two random samples from the poulation.\n",
    "\n",
    "Your task is:\n",
    "1. Lookup the equation for Gini Impurity Metric (e.g. at Wikipedia), and use it to complete the `compute_impurity` function.\n",
    " * `class_counts` can help you along the way (note that it returns a dictionary)\n",
    " * Use the **test cell** below to make sure your code runs correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_impurity(data):\n",
    "    \"\"\" Calculate the impurity using the Gini Impurity Metric.\n",
    "        \n",
    "        Write the function generally, such that it works with different numbers of classes.\n",
    "    \"\"\"\n",
    "    # determine number of classes in data (last column)\n",
    "    \n",
    "    pass ## YOUR CODE HERE\n"   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_impurity(data_class)\t\t 0.6666666666666665\n",
      "compute_impurity(data_class[:100])\t 0.585\n",
      "compute_impurity(data_class[:50])\t 0.17999999999999994\n"
     ]
    }
   ],
   "source": [
    "## Test cell\n",
    "## When you run this cell you should get the same output as in the next cell.\n",
    "print('compute_impurity(data_class)\\t\\t', compute_impurity(data_train))\n",
    "print('compute_impurity(data_class[:100])\\t', compute_impurity(data_train[:100]))\n",
    "print('compute_impurity(data_class[:50])\\t', compute_impurity(data_train[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test cell** correct output:\n",
    "\n",
    "    compute_impurity(data_class)\t\t 0.6666666666666665\n",
    "    compute_impurity(data_class[:100])\t 0.585\n",
    "    compute_impurity(data_class[:50])\t 0.17999999999999994\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Branching\n",
    "\n",
    "> Estimated time: 20 minutes.\n",
    "\n",
    "The next thing we need to be able to do is use the impurity measure to split the data.\n",
    "A split is based on a `if-then` rule, here called a `Question`.\n",
    "A question is associated with a coordinate $d\\in \\{1, ..., D\\}$ (what feature are we asking about), and a threshold, $\\theta$.\n",
    "E.g: if $x_d < \\theta$ go to left child, otherwise go to right child.\n",
    "\n",
    "Note that each feature is considered independently - therefore decision trees aren't affected by normalization.\n",
    "\n",
    "Your task is to:\n",
    "1. Finish the `match` method in the `Question` class. \n",
    " * Be sure that it can handle both categorican and numerical data (even though we only use numerical data here).\n",
    "1. Complete the `partition` function in the next cell.\n",
    " * Use `compute_impurity` and `Question` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\" A Question is used to partition a dataset.\n",
    "        This class just records a 'column number' (aka coordinate) and a\n",
    "        'column value' (aka threshold). The 'match' method is used to compare\n",
    "        the feature value in an example to the feature value stored in the\n",
    "        question. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "        self.value_is_numerical = is_numeric(self.value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Helper method to print in a readable format.\n",
    "        condition = \"==\"\n",
    "        if self.value_is_numerical:\n",
    "            condition = \">=\"\n",
    "        return \"Is %s %s %s?\" % (\n",
    "            iris.feature_names[self.column], condition, str(self.value))\n",
    "        \n",
    "    def match(self, example):\n",
    "        \"\"\" Takes a single example (a single row) and compares \n",
    "            it with the feature value in this question.\n",
    "            \n",
    "            Returns: Bool\n",
    "        \"\"\"\n",
    "        pass ## YOUR CODE HERE\n"   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(rows, question):\n",
    "    \"\"\" Partitions a dataset:\n",
    "        For each row in the dataset, check if it matches the question. \n",
    "        If so, add it to 'true rows', otherwise, add it to 'false rows'.\n",
    "        \n",
    "        Returns: true_rows, false_rows\n",
    "    \"\"\"\n",
    "    \n",
    "    true_rows, false_rows = [], []\n",
    "    \n",
    "    ## YOUR CODE HERE\n"   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Best Split\n",
    "\n",
    "> Estimated time: 20 minutes.\n",
    "\n",
    "Now that we know how to split we need to determine where to split!\n",
    "The best split is the one that maximizes the **information gain**, as determiend by the impurity measure.\n",
    "\n",
    "Your task is to:\n",
    "1. Complete the `find_best_split` function. \n",
    " * You will need to combine several things: `compute_impurity`, `Question`, `partition`, and `info_gain`.\n",
    " * Hint: Structure your code as a double for-loop such that you examine whether each value for each feature is a good threshold.\n",
    " * Remember to incorporate the `minimum_size` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(left, right, current_uncertainty):\n",
    "    \"\"\" Information Gain.\n",
    "        The uncertainty of the starting node, minus the weighted impurity of two child nodes.\n",
    "    \"\"\"\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_uncertainty - p * compute_impurity(left) - (1 - p) * compute_impurity(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(rows, minimum_size=5):\n",
    "    \"\"\" Find the best split by finding the information gain for each possible split.\n",
    "    \"\"\"\n",
    "    best_gain = 0  # keep track of the best information gain\n",
    "    best_question = None  # keep track of the feature / value that produced it\n",
    "    current_uncertainty = compute_impurity(rows)\n",
    "    n_features = len(rows[0]) - 1  # number of columns\n",
    "\n",
    "    ## YOUR CODE HERE\n"   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Growing Trees\n",
    "> Estimated time: 30 minutes.\n",
    "\n",
    "Now we have all the ingredients to start growing trees.\n",
    "The section after this has some functions that will run and test your functions.\n",
    "Have a look, and see if that works.\n",
    "First there are two helper functions that will help you get started.\n",
    "\n",
    "Your task is to:\n",
    "1. Complete the `build_tree` function.\n",
    " * It is suggested to do so using **recursion**.\n",
    " * Each iteration should create either a `Leaf_Node` or a `Decision_Node`.\n",
    "1. Complete the `classify` function.\n",
    " * The function should take **one** observation, and return the predicted class.\n",
    " * You based on the `Question` decide whether to follow the true-branch or the false-branch until you hit a `Leaf_Node`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "class Leaf_Node:\n",
    "    \"\"\" A Leaf_Node classifies data.\n",
    "        This holds a dictionary of class (e.g., \"Apple\") -> number of times\n",
    "        it appears in the rows from the training data that reach this leaf.\n",
    "    \"\"\"\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = class_counts(rows)\n",
    "\n",
    "class Decision_Node:\n",
    "    \"\"\" A Decision Node asks a question.\n",
    "        This holds a reference to the question, and to the two child nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 true_branch,\n",
    "                 false_branch):\n",
    "        \n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(rows):\n",
    "    \"\"\" Builds the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    pass ## YOUR CODE HERE\n"   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(row, node):\n",
    "    \"\"\" Recursively search\n",
    "    \"\"\"\n",
    "    \n",
    "    pass ## YOUR CODE HERE\n"   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Results\n",
    "\n",
    "Good job!\n",
    "Now comes the fun part.\n",
    "Run the cells below, and interpret the results.\n",
    "\n",
    "Note that the Iris dataset is a quite simple dataset, and you shouldn't expect to get nearly this good results in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, spacing=\"\"):\n",
    "    \"\"\" World's most elegant tree printing function.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf_Node):\n",
    "        print (spacing + \"Predict\", node.predictions)\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    print (spacing + str(node.question))\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    print (spacing + '--> True:')\n",
    "    print_tree(node.true_branch, spacing + \"|.\")\n",
    "\n",
    "    # Call this function recursively on the false branch\n",
    "    print (spacing + '--> False:')\n",
    "    print_tree(node.false_branch, spacing + \"|.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is petal width (cm) >= 0.6?\n",
      "--> True:\n",
      "|.Is petal width (cm) >= 1.7?\n",
      "|.--> True:\n",
      "|.|.Predict {2.0: 40}\n",
      "|.--> False:\n",
      "|.|.Is petal length (cm) >= 4.9?\n",
      "|.|.--> True:\n",
      "|.|.|.Predict {1.0: 2, 2.0: 4}\n",
      "|.|.--> False:\n",
      "|.|.|.Is sepal length (cm) >= 5.2?\n",
      "|.|.|.--> True:\n",
      "|.|.|.|.Predict {1.0: 39}\n",
      "|.|.|.--> False:\n",
      "|.|.|.|.Predict {1.0: 4, 2.0: 1}\n",
      "--> False:\n",
      "|.Predict {0.0: 45}\n"
     ]
    }
   ],
   "source": [
    "my_tree = build_tree(data_train)\n",
    "print_tree(my_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? True\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? False\n",
      "Is petal length (cm) >= 4.9? False\n",
      "Is sepal length (cm) >= 5.2? True\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? False\n",
      "Is petal length (cm) >= 4.9? False\n",
      "Is sepal length (cm) >= 5.2? True\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? True\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? False\n",
      "Is petal length (cm) >= 4.9? False\n",
      "Is sepal length (cm) >= 5.2? False\n",
      "Actual: 1.0. Predicted: {1.0: '80%', 2.0: '20%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? False\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? False\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? False\n",
      "Is petal length (cm) >= 4.9? False\n",
      "Is sepal length (cm) >= 5.2? True\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? False\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? True\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? True\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? True\n",
      "Actual: 1.0. Predicted: {2.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? True\n",
      "Is petal width (cm) >= 1.7? True\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? False\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "\n",
      "Is petal width (cm) >= 0.6? False\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_leaf(counts):\n",
    "    \"\"\"A nicer way to print the predictions at a leaf.\"\"\"\n",
    "    total = sum(counts.values()) * 1.0\n",
    "    probs = {}\n",
    "    for lbl in counts.keys():\n",
    "        probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
    "    return probs\n",
    "\n",
    "for row in data_test:\n",
    "    print (\"Actual: %s. Predicted: %s\" %\n",
    "           (row[-1], print_leaf(classify(row, my_tree))))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook is made heavily inspired and borrowing from: [Google Developers: Machine Learning Recipes #8](https://www.youtube.com/watch?v=LDRbO9a6XPU) ([source code](https://github.com/random-forests/tutorials/)).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
