{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Networks with Keras and TensorFlow\n",
    "\n",
    "TensorFlow is an open source software library for numerical computation that can be used for many things, but is mostly know for its use in machine learning and especially in deep learning.\n",
    "Since its release in 2015 it has quickly become one of the most popular and most actively developed libraries for deep learning.\n",
    "TensorFlow represents computations as graphs, which enables simple parallelization (as opposed to sequentially), and automatic differentiation.\n",
    "\n",
    "Pure TensorFlow is very verbose, and it is therefore a good idea to use a high-level API.\n",
    "Doing so simplifies and speeds-up development, reduces the risk of bugs, and generally reduces headache.\n",
    "The officially supported high-level API is **[Keras](https://keras.io/)**, and will be the focus of this lab.\n",
    "\n",
    "\n",
    "### External resources\n",
    "If you want a deeper dive the following are good places to start:\n",
    "\n",
    "* [Deep Learning course exercises from DTU](https://github.com/DeepLearningDTU/02456-deep-learning) - more hands on TensorFlow exercises.\n",
    "* [Official TensorFlow getting started material](https://www.tensorflow.org/get_started/) - collection of good tutorials from beginer to quite advanced.\n",
    "* [Official Keras getting started marerial](https://keras.io/getting-started/functional-api-guide/)\n",
    "* [API documentation](https://www.tensorflow.org/api_docs/python/) - Most of the documentation for TF is written into the code, so the best way to figure out how somethings works is often to look it up in the API, and then look at the implementation. The [API guides](https://www.tensorflow.org/api_guides/python/array_ops) can also be very useful sometimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dependancies and supporting functions by running the code block below.\n",
    "from __future__ import absolute_import, division, print_function \n",
    "\n",
    "import time\n",
    "import sys, os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "For this exercise we will use the [fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset.\n",
    "Like the classical MNIST data set it consists of images `28x28` grayscale images of of 10 different classes, and the objective is to correctly classify as many of them as possible.\n",
    "Fashion-MNIST is however more challenging (though still a toy-dataset), making it more interesting to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load data\n",
    "mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train_),(x_test, y_test_) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize data to [0, 1] interval\n",
    "\n",
    "## Print dataset statistics and visualize\n",
    "print(\"\"\"Information on dataset\n",
    "----------------------\"\"\")\n",
    "print(\"Training data shape:\\t\", x_train.shape, y_train_.shape)\n",
    "print(\"Test data shape\\t\\t\", x_test.shape, y_test_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST consists of images of 10 different types of clothing.\n",
    "The labels are:\n",
    "\n",
    "| Label:       | 0           | 1       | 2        | 3     |    4 | 5     | 6      | 7      | 8   |   9 |\n",
    "| -| -| -| -| -| -| -| -| -| -| -|\n",
    "| **Description:** | T-shirt/top | Trouser | Pullover | Dress | Coat | Sandal | Shirt | Sneaker | Bag | Ankle boot |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot a few MNIST examples\n",
    "img_to_show = 5\n",
    "idx = 0\n",
    "canvas = np.zeros((28*img_to_show, img_to_show*28))\n",
    "print('\\nLabels')\n",
    "for i in range(img_to_show):\n",
    "    for j in range(img_to_show):\n",
    "        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_train[idx]#mnist_data.train.images[idx].reshape((28, 28))\n",
    "        print(y_train_[idx], end=', ')\n",
    "        idx += 1\n",
    "    print()\n",
    "\n",
    "print('\\nInput data')\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.axis('off')\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('fashion-MNIST data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "Class labels for neural netowrks (almost) always use **one-hot encoding**.\n",
    "Rather than using the integers directly they are encoded as binary vectors with one bit with the value `1`, and the rest `0`.\n",
    "The position of the `1` indicates the label.\n",
    "The code below converts the integer labels to one-hot labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = OneHotEncoder(categories='auto', sparse=False).fit_transform(y_train_[:,None])\n",
    "y_test = OneHotEncoder(categories='auto', sparse=False).fit_transform(y_test_[:,None])\n",
    "\n",
    "for i in range(10):\n",
    "    print(y_train_[i], '-->', y_train[i,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of Learning\n",
    "Many parts of deep learning are still more art than science.\n",
    "When defining and training a deep network there are many options, and no proven method of finding the best (or a good) configuration.\n",
    "Hyperparameters can be found by experience (guessing) or some search procedure. \n",
    "Random search is easy to implement and performs decently: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf.\n",
    "More advanced search procedures include [SPEARMINT](https://github.com/JasperSnoek/spearmint) and many others.\n",
    "\n",
    "Some important factors, and good starting points are given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ballpark estimates of hyperparameters\n",
    "__Number of hidden units and network architecture__\n",
    "* Probably as big network as possible (memory and time constraints) and then apply regularization. \n",
    "   You'll have to experiment :). \n",
    "   One rarely goes below 256 units for feedforward networks unless your are training on CPU...\n",
    "   Theres is some research into stochastic depth networks: https://arxiv.org/pdf/1603.09382v2.pdf, but in general this is trial and error.\n",
    "   \n",
    "   \n",
    "__Loss function__\n",
    "The [loss function](https://keras.io/losses/) is typically determiend by the problem, but there is some flexibility here as well.\n",
    "Broadly speaking we use:\n",
    " * Cross entropy for classification (discrete targets)\n",
    " * Mean squared error for regression (continuous targets)\n",
    " \n",
    "__Parameter initialization__\n",
    "    [Parameter initialization](](https://keras.io/initializers/)) is extremely important. There are a lot of different initializers. Often used initializer are\n",
    "    1. He\n",
    "    2. Glorot\n",
    "    3. Uniform or Normal with small scale. (0.1 - 0.01)\n",
    "    4. Orthogonal (works well for RNNs, somtimes)\n",
    "\n",
    "Bias is nearly always initialized to zero.\n",
    "   \n",
    "__Nonlinearity__: The most commonly used [nonliearities](https://keras.io/activations/) are:    \n",
    "    1. ReLU\n",
    "    2. Leaky ReLU\n",
    "    3. Elu\n",
    "    3. Sigmoids are used if your output is binary. It is not used in the hidden layers. Squases the output between -1 and 1\n",
    "    4. Softmax used as output if you have a classification problem. Normalizes the the output to 1. )\n",
    "\n",
    "__Regularization:__\n",
    "    1. Dropout: Dropout rate 0.1-0.5\n",
    "    2. [L2 and L1 regularization](https://keras.io/regularizers/): 1e-4 - 1e-8.\n",
    "    3. [Batchnorm](https://keras.io/layers/normalization/): Batchnorm also acts as a regularizer.\n",
    "    Often very useful (faster and better convergence)\n",
    "    4. Early stopping: Very frequently used.\n",
    "    \n",
    "__Optimizers:__\n",
    "    1. SGD + Momentum: learning rate 1.0 - 0.1 \n",
    "    2. ADAM: learning rate 3*1e-4 - 1e-5\n",
    "    3. RMSPROP: somewhere between SGD and ADAM\n",
    "\n",
    "__mini-batch size__\n",
    "* Usually people use 16-256. Bigger is not allways better. With smaller mini-batch size you get more updates and your model might converge faster. Also small batchsizes uses less memory -> you can train a model with more parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to hold experiments - but don't overwrite, even if we re-run this cell.\n",
    "try:\n",
    "    experiments\n",
    "except NameError:\n",
    "    experiments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "def visualize_experiments(experiments):\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    for experiment in experiments:\n",
    "        exp_name, _, info = experiment\n",
    " \n",
    "        ax = plt.subplot(\"211\")\n",
    "        ax.set_title('Validation Accuracy')\n",
    "        plt.plot(info.history['val_acc'], label=exp_name)\n",
    "        plt.legend()\n",
    "\n",
    "        ax = plt.subplot(\"212\")\n",
    "        ax.set_title('Validation Loss')\n",
    "        plt.plot(info.history['val_loss'], label=exp_name)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_info(experiment):\n",
    "    name, _, info = experiment\n",
    "    print('Params:')\n",
    "    for key in info.params:\n",
    "        print('{:20}'.format(key), info.params[key])\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = plt.subplot(\"211\")\n",
    "    ax.set_title('Accuracy: '+ name)\n",
    "    plt.plot(info.history['val_acc'], label='val_acc')\n",
    "    plt.plot(info.history['acc'], label='train_acc')\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.subplot(\"212\")\n",
    "    ax.set_title('Loss: ' + name)\n",
    "    plt.plot(info.history['val_loss'], label='val_loss')\n",
    "    plt.plot(info.history['loss'], label='loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def keep_best(experiments, n):\n",
    "    \"\"\" Return the n best experiments.\"\"\"\n",
    "    if len(experiments) < n:\n",
    "        return experiments\n",
    "    \n",
    "    exp_sorted = sorted(experiments, key=lambda x: np.max(x[2].history['val_acc']), reverse=True)\n",
    "    return exp_sorted[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, loss, optimizer, num_epochs, exp_name=None, use_tensorboard=False):    \n",
    "    exp_name = exp_name or 'log_{:.0f}'.format(time.time()*100)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    if use_tensorboard:\n",
    "        tensorboard = TensorBoard(log_dir='logdir/'+exp_name)\n",
    "        fit_info = model.fit(x_train, y_train, epochs=num_epochs, batch_size=256, validation_split=0.1, callbacks=[tensorboard])\n",
    "    else:\n",
    "        fit_info = model.fit(x_train, y_train, epochs=num_epochs, batch_size=256, validation_split=0.1)\n",
    "    return exp_name, model, fit_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Task\n",
    "is to create the best network as possible, and attain the highest validation performance you can.\n",
    "Once you are happy you can test your model on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple **loss functions** available, but for classification we almost always use [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.backend.categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also several optimizers (gradient descent algorithms) to choose from.\n",
    "Stochastic Gradient Descent (SGD) is the classical one, and it is still used frequently.\n",
    "There are however [cases where it doesn't perform so well](https://imgur.com/a/Hqolp) and another more advanced algorithm is better suited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Define the model we want to train\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # Dense layers only work with feature vectors, so we need to flatten first\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "\n",
    "    # A classic layer consists of 4 parts:\n",
    "    keras.layers.Dense(256),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    # The output layer must have the same number of units as there are classes\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "print('\\nBegin Training')\n",
    "num_epochs = 10\n",
    "exp_name = 'default_network'\n",
    "experiment = train(model, loss_function, optimizer, num_epochs, exp_name=exp_name)\n",
    "experiments.append(experiment)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_info(experiment)\n",
    "visualize_experiments(experiments)\n",
    "\n",
    "for experiment in experiments:\n",
    "    name, _, info = experiment\n",
    "    print(name, np.max(info.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If applicable - clean up your experiments list a bit\n",
    "experiments = keep_best(experiments, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Ideally you would only ever use test data once - when you are completely 100% done and satisfied with your model.\n",
    "This is rarely the practice in real life, but you should keep some discipline, and not use the test too often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_you_happy_about_your_model = False\n",
    "\n",
    "if are_you_happy_about_your_model:\n",
    "    best_experimnet = keep_best(experiments, 1)[0]\n",
    "    print('Best model test loss and accuracy:')\n",
    "    print(best_experimnet[1].evaluate(x_test, y_test, batch_size=128))\n",
    "    print()\n",
    "\n",
    "    visualize_info(best_experimnet)\n",
    "else:\n",
    "    print('Come back when you are happy.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
